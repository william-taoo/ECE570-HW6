{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Setup and Tokenizer Plan**\n",
    "\n",
    "**Data Plan**\n",
    "- I will download the Tiny Shakespeare dataset with this raw URL: `https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt`\n",
    "- I'll split the dataset into train and test sets, and produce context target pairs where there are labels that represent the next token for each position.\n",
    "\n",
    "**Tokenizer Plan**\n",
    "We'll compare the following tokenizers:\n",
    "1. **BPE** using HuggingFace's `tokenizers` library\n",
    "2. **SentencePiece** \n",
    "3. **Character-level** from scratch\n",
    "\n",
    "I chose these tokenizers because BPE gives good subword units and works well in many datasets. SentencePiece performs well and is popular. Character-level is a simple baseline that generally well.\n",
    "\n",
    "**Training Plan**\n",
    "- Epochs: 20 - might depend on the time\n",
    "- Optimizer: `Adam`\n",
    "- Learning Rate: start with 1e-3 and have a scheduler option\n",
    "- Batch size: 16\n",
    "- Context length: 128\n",
    "- Loss: `nn.CrossEntropyLoss` with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data, Tokenizers, and Training Functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, processors\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "data_path = Path(\"tiny_shakespeare_input.txt\")\n",
    "if not data_path.exists():\n",
    "    print(\"Downloading Tiny Shakespeare dataset\")\n",
    "    urllib.request.urlretrieve(data_url, data_path)\n",
    "else:\n",
    "    print(\"Dataset already present.\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length (chars):\", len(text))\n",
    "\n",
    "# Train/Test split\n",
    "val_ratio = 0.2\n",
    "val_size = int(val_ratio * len(text))\n",
    "train_text = text[:-val_size]\n",
    "val_text = text[-val_size:]\n",
    "\n",
    "# Character-level Tokenizers\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.id2token = text\n",
    "        self.token2id = {ch:i for i, ch in enumerate(self.id2token)}\n",
    "        self.vocab_size = len(self.id2token)\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.token2id[c] for c in s]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.id2token[i] for i in ids])\n",
    "    \n",
    "# BPE\n",
    "def train_bpe(text, vocab_size, path):\n",
    "    file = \"bpe_text.txt\"\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "    tokenizer.train([file], trainer)\n",
    "    tokenizer.model.save(\".\", \"bpe-tokenizer\")\n",
    "    tok = Tokenizer.from_file(\"bpe-tokenizer.json\")\n",
    "    return tok\n",
    "\n",
    "# SentencePiece\n",
    "def train_sentencepiece(text, prefix, vocab_size, model_type):\n",
    "    file = \"spm_text.txt\"\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file)\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=file,\n",
    "        model_prefix=prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=model_type,\n",
    "        unk_id=0,\n",
    "        pad_id=1,\n",
    "        bos_id=-1,\n",
    "        eos_id=-1\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{prefix}.model\")\n",
    "    return sp\n",
    "\n",
    "BPE_VOCAB = 3000\n",
    "SPM_VOCAB = 3000\n",
    "\n",
    "print(\"Training BPE tokenizer\")\n",
    "bpe_tok = train_bpe(train_text, vocab_size=BPE_VOCAB, path=\"bpe-tokenizer.json\")\n",
    "\n",
    "print(\"Training SentencePiece tokenizer\")\n",
    "spm_tok = train_sentencepiece(train_text, prefix=\"spm\", vocab_size=SPM_VOCAB, model_type=\"bpe\")\n",
    "\n",
    "print(\"Building char tokenizer\")\n",
    "char_tok = CharTokenizer(text)\n",
    "\n",
    "print(\"Vocabulary sizes:\")\n",
    "try:\n",
    "    print(\"BPE (tokenizers) vocab size:\", bpe_tok.get_vocab_size())\n",
    "except Exception as e:\n",
    "    print(\"BPE vocab error:\", e)\n",
    "print(\"SentencePiece vocab size:\", spm_tok.get_piece_size())\n",
    "print(\"Char vocab size:\", char_tok.vocab_size)\n",
    "\n",
    "# Tokenize\n",
    "example = \"Hello, world!\"\n",
    "print(\"Example text:\", example)\n",
    "\n",
    "# Char\n",
    "char_ids = char_tok.encode(example)\n",
    "print(\"Char ids:\", char_ids)\n",
    "print(\"Char decoded:\", char_tok.decode(char_ids))\n",
    "\n",
    "# BPE\n",
    "bpe_out = bpe_tok.encode(example)\n",
    "print(\"BPE tokenizer ids:\", bpe_out.ids)\n",
    "print(\"BPE tokenizer tokens:\", bpe_out.tokens)\n",
    "print(\"BPE decoded string:\", bpe_tok.decode(bpe_out.ids))\n",
    "\n",
    "# SentencePiece\n",
    "spm_ids = spm_tok.EncodeAsIds(example)\n",
    "spm_pieces = spm_tok.EncodeAsPieces(example)\n",
    "print(\"SentencePiece ids:\", spm_ids)\n",
    "print(\"SentencePiece pieces:\", spm_pieces)\n",
    "print(\"SentencePiece decoded string:\", spm_tok.DecodeIds(spm_ids))\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, token_ids, context_length):\n",
    "        self.ids = token_ids\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.ids) - self.context_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.ids[idx: idx + self.context_length]\n",
    "        y = self.ids[idx + 1: idx + self.context_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def build_token_ids(tokenizer_type, tokenizer, text):\n",
    "    if tokenizer_type == \"char\":\n",
    "        ids = tokenizer.encode(text)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        ids = bpe_tok.encode(text).ids\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        ids = spm_tok.EncodeAsIds(text)\n",
    "    else:\n",
    "        print(\"Invalid tokenizer type:\", tokenizer_type)\n",
    "        return None\n",
    "    return ids\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs, print_every=100):\n",
    "    model.to(device)\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (xb, yb) in enumerate(train_loader):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B*S, V), yb.view(B*S))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        history[\"val_loss\"].append((epoch, val_loss))\n",
    "        print(f\"Epoch {epoch+1} validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B*S, V), yb.view(B*S))\n",
    "            total += loss.item() * xb.size(0)\n",
    "            count += xb.size(0)\n",
    "    return total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Positional Encoding (From Scratch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model, dtype=torch.float32)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Even\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Odd\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "    \n",
    "d_model = 64\n",
    "max_seq_len = 256\n",
    "pos_enc = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "pe_mat = pos_enc.pe\n",
    "def print_pe_value(pos, dim):\n",
    "    v = float(pe_mat[pos, dim].item())\n",
    "    print(f\"pos={pos}, dim={dim} => {v:.10f}\")\n",
    "\n",
    "print_pe_value(5, 10)\n",
    "print_pe_value(5, 15)\n",
    "print_pe_value(100, 20)\n",
    "print_pe_value(100, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Transformer Building Blocks (From Scratch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.act = activation()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.ln(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return residual + out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def causal_mask(self, seq_len, device):\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, S, _ = x.size()\n",
    "        residual = x\n",
    "        x_norm = self.ln(x)\n",
    "\n",
    "        q = self.q_proj(x_norm)\n",
    "        k = self.k_proj(x_norm)\n",
    "        v = self.v_proj(x_norm)\n",
    "\n",
    "        q = q.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        causal = self.causal_mask(S, x.device)\n",
    "        scores = scores.masked_fill(causal == 0, float(\"-inf\"))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, S, self.d_model)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return residual + out\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    B, S, d_model, heads = 2, 8, 64, 8\n",
    "    x = torch.randn(B, S, d_model)\n",
    "    att = MultiHeadAttention(d_model=d_model, n_heads=heads)\n",
    "    y = att(x)\n",
    "    print(\"Attention output shape:\", y.shape)\n",
    "    ff = FeedForward(d_model=d_model, d_ff=4*d_model)\n",
    "    z = ff(y)\n",
    "    print(\"FF output shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Transformer Implementation and Training\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(x)\n",
    "        x = self.ff(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=8, d_ff=None, context_length=128, dropout=0.1, pos_enc_module=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = pos_enc_module if pos_enc_module is not None else SinusoidalPositionalEncoding(d_model, max_seq_len=context_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            self.blocks.append(Decoder(d_model, n_heads, d_ff, dropout=dropout))\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, S = idx.shape\n",
    "        assert S <= self.context_length, f\"Sequence length too long: {S} > {self.context_length}\"\n",
    "        token_embedding = self.token_embedding(idx)\n",
    "        pos = self.pos_enc(torch.zeros(B, S, self.d_model, device=token_embedding.device))\n",
    "        x = self.dropout(token_embedding + pos)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# Hyperparameters\n",
    "context_length = 128\n",
    "d_model = 256\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "vocab_for_model = bpe_tok.get_vocab_size()\n",
    "print(\"Using vocab size for model:\", vocab_for_model)\n",
    "\n",
    "pos_module = SinusoidalPositionalEncoding(d_model=d_model, max_seq_len=context_length)\n",
    "\n",
    "model = TransformerDecoder(\n",
    "    vocab_size=vocab_for_model,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=4*d_model,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout,\n",
    "    pos_enc_module=pos_module\n",
    ").to(device)\n",
    "\n",
    "# Prepare training data tokens (use BPE tokens in this run)\n",
    "ids_train = build_token_ids(\"bpe\", tokenizer=bpe_tok, text=train_text)\n",
    "ids_val = build_token_ids(\"bpe\", tokenizer=bpe_tok, text=val_text)\n",
    "\n",
    "train_dataset = CharDataset(ids_train, context_length=context_length)\n",
    "val_dataset = CharDataset(ids_val, context_length=context_length)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=3, print_every=50)\n",
    "\n",
    "final_val = evaluate_model(model, val_loader, criterion, device)\n",
    "print(\"Final validation loss:\", final_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Generation and Sampling Plan**\n",
    "\n",
    "**Prompt**: I'll use `O Romeo, Romeo!` as my prompt\n",
    "\n",
    "**Parameters to test**\n",
    "- **Temperature:** T = 0.2 and T = 1.0\n",
    "- **Top-k** k = 5 and k = 50\n",
    "- **Top-p**p = 0.6 and p = 0.9\n",
    "\n",
    "**Hypothesis**\n",
    "- Temperature 0.2: the generations will be repetitive and conservative\n",
    "- Temperature 1.0: more varied\n",
    "- Top-k small (5): might get stuck in loops\n",
    "- Top-k 50: more variety, more creative\n",
    "- Top-p 0.6: conservative, coherent but safe\n",
    "- Top-p 0.9: more creative and human-like, but may output interesting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generation and Sampling Implementation\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def sample_temperature(logits, T):\n",
    "    logits = logits / (T + 1e-12)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    return idx\n",
    "\n",
    "def sample_top_k(logits, k):\n",
    "    if k <= 0:\n",
    "        return sample_temperature(logits, 1.0)\n",
    "    \n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.zeros_like(logits).to(logits.device)\n",
    "    probs[indices] = F.softmax(values, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    return idx\n",
    "\n",
    "def sample_top_p(logits, p):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    probs = F.softmax(sorted_logits, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "    cutoff = torch.searchsorted(cumulative_probs, p)\n",
    "    cutoff = min(cutoff.item()+1, logits.size(0))\n",
    "    probs_to_sample = probs[:cutoff]\n",
    "    indices_to_sample = sorted_indices[:cutoff]\n",
    "    probs_to_sample = probs_to_sample / probs_to_sample.sum()\n",
    "    chosen_index = torch.multinomial(probs_to_sample, num_samples=1).item()\n",
    "\n",
    "    return int(indices_to_sample[chosen_index].item())\n",
    "\n",
    "def generate_autoregressive(model, tokenizer_type, tokenizer, prompt, max_new_tokens, method=\"temperature\", method_param=1.0, device=device):\n",
    "    # Convert prompt to token ids using specified tokenizer\n",
    "    if tokenizer_type == \"char\":\n",
    "        ids = tokenizer.encode(prompt)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        ids = bpe_tok.encode(prompt).ids\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        ids = spm_tok.EncodeAsIds(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown tokenizer_type\")\n",
    "\n",
    "    model.eval()\n",
    "    cur_ids = ids.copy()\n",
    "    for _ in range(max_new_tokens):\n",
    "        input_ids = cur_ids[-model.context_length:]\n",
    "        x = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "        logits = model(x)\n",
    "        last_logits = logits[0, -1, :].detach().cpu()\n",
    "\n",
    "        if method == \"temperature\":\n",
    "            next_id = sample_temperature(last_logits, method_param)\n",
    "        elif method == \"top_k\":\n",
    "            next_id = sample_top_k(last_logits, int(method_param))\n",
    "        elif method == \"top_p\":\n",
    "            next_id = sample_top_p(last_logits, float(method_param))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampling method\")\n",
    "        cur_ids.append(next_id)\n",
    "\n",
    "    if tokenizer_type == \"char\":\n",
    "        out_text = tokenizer.decode(cur_ids)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        out_text = bpe_tok.decode(cur_ids)\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        out_text = spm_tok.DecodeIds(cur_ids)\n",
    "\n",
    "    return out_text\n",
    "\n",
    "# Generate examples for each method and parameter set\n",
    "seed_prompt = \"O Romeo, Romeo!\"\n",
    "print(\"Seed prompt:\", seed_prompt)\n",
    "\n",
    "print(\"\\n--- Temperature T=0.2 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"temperature\", method_param=0.2))\n",
    "\n",
    "print(\"\\n--- Temperature T=1.0 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"temperature\", method_param=1.0))\n",
    "\n",
    "print(\"\\n--- Top-k k=5 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_k\", method_param=5))\n",
    "\n",
    "print(\"\\n--- Top-k k=50 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_k\", method_param=50))\n",
    "\n",
    "print(\"\\n--- Top-p p=0.6 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_p\", method_param=0.6))\n",
    "\n",
    "print(\"\\n--- Top-p p=0.9 ---\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_p\", method_param=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 8: Analysis and Discussion**\n",
    "\n",
    "**Tokenizer Comparison**\n",
    "- **Char-level**: smallest implementation compelxity and least requirements. Vocabulary is equal to the number of unique characters. Tokenization is trivial but sequence lengths are long, making modeling potentially slower.\n",
    "- **BPE**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
