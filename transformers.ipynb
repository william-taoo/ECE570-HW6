{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 1: Setup and Tokenizer Plan**\n",
    "\n",
    "**Data Plan**\n",
    "- I will download the Tiny Shakespeare dataset with this raw URL: `https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt`\n",
    "- I'll split the dataset into train and test sets, and produce context target pairs where there are labels that represent the next token for each position.\n",
    "\n",
    "**Tokenizer Plan**\n",
    "We'll compare the following tokenizers:\n",
    "1. **BPE** using HuggingFace's `tokenizers` library\n",
    "2. **SentencePiece** \n",
    "3. **Character-level** from scratch\n",
    "\n",
    "I chose these tokenizers because BPE gives good subword units and works well in many datasets. SentencePiece performs well and is popular. Character-level is a simple baseline that generally well.\n",
    "\n",
    "**Training Plan**\n",
    "- Epochs: 20 - might depend on the time\n",
    "- Optimizer: `Adam`\n",
    "- Learning Rate: start with 1e-3 and have a scheduler option\n",
    "- Batch size: 16\n",
    "- Context length: 128\n",
    "- Loss: `nn.CrossEntropyLoss` with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dataset already present.\n",
      "Length (chars): 1115394\n",
      "Training BPE tokenizer\n",
      "\n",
      "\n",
      "\n",
      "Training SentencePiece tokenizer\n",
      "Building char tokenizer\n",
      "Vocabulary sizes:\n",
      "BPE (tokenizers) vocab size: 3000\n",
      "SentencePiece vocab size: 3000\n",
      "Char vocab size: 1115394\n",
      "Example text: Hello, world!\n",
      "Char ids: [1112021, 1115374, 1115373, 1115373, 1115379, 1115352, 1115385, 1115386, 1115379, 1115383, 1115373, 1115349, 1114532]\n",
      "Char decoded: Hello, world!\n",
      "BPE tokenizer ids: [315, 2658, 6, 645, 2]\n",
      "BPE tokenizer tokens: ['He', 'llo', ',', 'world', '!']\n",
      "BPE decoded string: He llo , world !\n",
      "SentencePiece ids: [70, 178, 2944, 2956, 668, 2986]\n",
      "SentencePiece pieces: ['▁H', 'ell', 'o', ',', '▁world', '!']\n",
      "SentencePiece decoded string: Hello, world!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: spm_text.txt\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: spm_text.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 25963 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=886765\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9658% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999658\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 25963 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 25963\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 22075\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19709 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4892 size=20 all=1582 active=1522 piece=it\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3116 size=40 all=2179 active=2119 piece=▁he\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1933 size=60 all=2872 active=2812 piece=ld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1499 size=80 all=3493 active=3433 piece=▁is\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1158 size=100 all=3939 active=3879 piece=ome\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1153 min_freq=78\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1010 size=120 all=4571 active=1615 piece=▁him\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=792 size=140 all=5059 active=2103 piece=▁do\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=659 size=160 all=5537 active=2581 piece=���our\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=577 size=180 all=5939 active=2983 piece=▁are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=505 size=200 all=6386 active=3430 piece=sel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=502 min_freq=72\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=454 size=220 all=6692 active=1284 piece=end\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=420 size=240 all=7098 active=1690 piece=are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=379 size=260 all=7435 active=2027 piece=▁My\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=347 size=280 all=7737 active=2329 piece=▁or\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=319 size=300 all=8254 active=2846 piece=ame\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=319 min_freq=62\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=292 size=320 all=8535 active=1262 piece=ICH\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=340 all=8679 active=1406 piece=▁upon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=254 size=360 all=8975 active=1702 piece=ick\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=237 size=380 all=9130 active=1857 piece=▁fri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=400 all=9256 active=1983 piece=ack\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=224 min_freq=56\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=212 size=420 all=9455 active=1174 piece=ign\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=440 all=9620 active=1339 piece=▁HENRY\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=460 all=9837 active=1556 piece=ORK\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=480 all=10013 active=1732 piece=▁Come\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=500 all=10145 active=1864 piece=urn\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=171 min_freq=49\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=163 size=520 all=10257 active=1092 piece=▁Edward\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=540 all=10351 active=1186 piece=▁stand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=560 all=10573 active=1408 piece=▁been\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=580 all=10716 active=1551 piece=old\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=600 all=10872 active=1707 piece=▁down\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=136 min_freq=43\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=620 all=11024 active=1150 piece=land\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=640 all=11129 active=1255 piece=ULI\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=660 all=11351 active=1477 piece=aint\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=680 all=11438 active=1564 piece=▁leave\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=700 all=11538 active=1664 piece=▁prince\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=111 min_freq=40\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=720 all=11691 active=1153 piece=▁im\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=740 all=11869 active=1331 piece=▁Go\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=100 size=760 all=11988 active=1450 piece=sw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=97 size=780 all=12138 active=1600 piece=for\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=96 size=800 all=12322 active=1784 piece=▁people\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=95 min_freq=36\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=93 size=820 all=12431 active=1110 piece=▁pri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=91 size=840 all=12562 active=1241 piece=▁little\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=89 size=860 all=12669 active=1348 piece=▁dear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=86 size=880 all=12757 active=1436 piece=▁CL\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84 size=900 all=12886 active=1565 piece=▁All\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=84 min_freq=32\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82 size=920 all=12980 active=1094 piece=▁forth\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=80 size=940 all=13084 active=1198 piece=▁mind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=78 size=960 all=13225 active=1339 piece=▁law\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=76 size=980 all=13346 active=1460 piece=▁last\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=73 size=1000 all=13419 active=1533 piece=▁bes\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=73 min_freq=29\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=71 size=1020 all=13464 active=1038 piece=GE\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69 size=1040 all=13541 active=1115 piece=▁thousand\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=67 size=1060 all=13613 active=1187 piece=▁bring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=65 size=1080 all=13663 active=1237 piece=▁imp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=1100 all=13732 active=1306 piece=▁Clown\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=27\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=1120 all=13845 active=1114 piece=ount\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=61 size=1140 all=13907 active=1176 piece=▁reven\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=59 size=1160 all=13992 active=1261 piece=imes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=1180 all=14045 active=1314 piece=NIA\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=1200 all=14120 active=1389 piece=▁Gentle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=57 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=1220 all=14185 active=1063 piece=▁comfort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=55 size=1240 all=14269 active=1147 piece=▁sleep\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=1260 all=14315 active=1193 piece=FF\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=53 size=1280 all=14385 active=1263 piece=morrow\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=1300 all=14425 active=1303 piece=essenger\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=52 min_freq=23\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=1320 all=14489 active=1063 piece=▁Clif\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=1340 all=14525 active=1099 piece=▁play\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=1360 all=14607 active=1181 piece=▁oath\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=1380 all=14655 active=1229 piece=eft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=1400 all=14737 active=1311 piece=hal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=45 min_freq=21\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=45 size=1420 all=14769 active=1030 piece=▁enough\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=1440 all=14845 active=1106 piece=up\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=1460 all=14944 active=1205 piece=▁Senator\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=1480 all=15002 active=1263 piece=▁grant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=1500 all=15030 active=1291 piece=ush\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1520 all=15123 active=1084 piece=earn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=1540 all=15152 active=1113 piece=▁Gloucester\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=39 size=1560 all=15255 active=1216 piece=▁grow\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1580 all=15333 active=1294 piece=cess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=1600 all=15381 active=1342 piece=▁counsel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=38 min_freq=19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=1620 all=15435 active=1054 piece=▁quick\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1640 all=15492 active=1111 piece=▁fur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1660 all=15557 active=1176 piece=eal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1680 all=15640 active=1259 piece=▁lose\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1700 all=15684 active=1303 piece=▁inc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=34 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1720 all=15731 active=1038 piece=PS\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1740 all=15807 active=1114 piece=▁issue\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1760 all=15866 active=1173 piece=▁min\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1780 all=15887 active=1194 piece=▁poison\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1800 all=15942 active=1249 piece=▁quar\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=31 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1820 all=15965 active=1021 piece=ub\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1840 all=16053 active=1109 piece=▁gave\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1860 all=16063 active=1119 piece=hee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1880 all=16135 active=1191 piece=▁next\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1900 all=16164 active=1220 piece=▁deliver\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=29 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1920 all=16259 active=1095 piece=owers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=27 size=1940 all=16267 active=1103 piece=sc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1960 all=16370 active=1206 piece=▁got\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1980 all=16387 active=1223 piece=▁watch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=2000 all=16430 active=1266 piece=▁buy\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=2020 all=16457 active=1027 piece=▁given\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=2040 all=16483 active=1053 piece=ITA\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=2060 all=16585 active=1155 piece=▁exc\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=2080 all=16628 active=1198 piece=▁soft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=2100 all=16651 active=1221 piece=ler\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=14\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=2120 all=16750 active=1086 piece=▁slew\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=2140 all=16760 active=1096 piece=▁consent\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=2160 all=16859 active=1195 piece=▁Bes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=2180 all=16888 active=1224 piece=▁PARIS\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=2200 all=16893 active=1229 piece=▁morning\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=23 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=2220 all=16994 active=1102 piece=▁cle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=2240 all=17068 active=1176 piece=▁Gaunt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=2260 all=17079 active=1187 piece=ENT\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=2280 all=17181 active=1289 piece=▁DOR\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=2300 all=17235 active=1343 piece=▁wert\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=21 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=2320 all=17228 active=994 piece=▁LARTIUS\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=2340 all=17273 active=1039 piece=▁du\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=2360 all=17334 active=1100 piece=▁VINC\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=2380 all=17350 active=1116 piece=▁Keeper\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=2400 all=17342 active=1108 piece=▁sentence\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=2420 all=17385 active=1043 piece=bury\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=2440 all=17447 active=1105 piece=▁John\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=2460 all=17467 active=1125 piece=▁ignor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=2480 all=17466 active=1124 piece=▁desperate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=2500 all=17550 active=1208 piece=▁dog\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=2520 all=17597 active=1048 piece=▁damn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=2540 all=17602 active=1053 piece=▁quiet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=2560 all=17595 active=1046 piece=▁servant\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2580 all=17615 active=1066 piece=ked\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2600 all=17712 active=1163 piece=rent\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2620 all=17785 active=1066 piece=arted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2640 all=17848 active=1129 piece=asters\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2660 all=17851 active=1132 piece=▁steal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=2680 all=17847 active=1128 piece=▁account\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=2700 all=17870 active=1151 piece=vel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=2720 all=17948 active=1074 piece=▁low\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=2740 all=17981 active=1107 piece=▁disg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=2760 all=18002 active=1128 piece=▁blame\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=2780 all=18008 active=1134 piece=▁double\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2800 all=18029 active=1155 piece=ua\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2820 all=18108 active=1076 piece=vish\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2840 all=18155 active=1123 piece=ursed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2860 all=18163 active=1131 piece=▁year\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2880 all=18169 active=1137 piece=▁going\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2900 all=18162 active=1130 piece=▁oracle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2920 all=18149 active=988 piece=▁lordship\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: spm.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data, Tokenizers, and Training Functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, processors\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "data_path = Path(\"tiny_shakespeare_input.txt\")\n",
    "if not data_path.exists():\n",
    "    print(\"Downloading Tiny Shakespeare dataset\")\n",
    "    urllib.request.urlretrieve(data_url, data_path)\n",
    "else:\n",
    "    print(\"Dataset already present.\")\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length (chars):\", len(text))\n",
    "\n",
    "# Train/Test split\n",
    "val_ratio = 0.2\n",
    "val_size = int(val_ratio * len(text))\n",
    "train_text = text[:-val_size]\n",
    "val_text = text[-val_size:]\n",
    "\n",
    "# Character-level Tokenizers\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.id2token = text\n",
    "        self.token2id = {ch:i for i, ch in enumerate(self.id2token)}\n",
    "        self.vocab_size = len(self.id2token)\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.token2id[c] for c in s]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return \"\".join([self.id2token[i] for i in ids])\n",
    "    \n",
    "# BPE\n",
    "def train_bpe(text, vocab_size, path):\n",
    "    file = \"bpe_text.txt\"\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.NFKC()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "    tokenizer.train([file], trainer)\n",
    "    tokenizer.save(path)\n",
    "    tok = Tokenizer.from_file(path)\n",
    "    return tok\n",
    "\n",
    "# SentencePiece\n",
    "def train_sentencepiece(text, prefix, vocab_size, model_type):\n",
    "    file = \"spm_text.txt\"\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=file,\n",
    "        model_prefix=prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=model_type,\n",
    "        unk_id=0,\n",
    "        pad_id=1,\n",
    "        bos_id=-1,\n",
    "        eos_id=-1\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f\"{prefix}.model\")\n",
    "    return sp\n",
    "\n",
    "BPE_VOCAB = 3000\n",
    "SPM_VOCAB = 3000\n",
    "\n",
    "print(\"Training BPE tokenizer\")\n",
    "bpe_tok = train_bpe(train_text, vocab_size=BPE_VOCAB, path=\"bpe-tokenizer.json\")\n",
    "\n",
    "print(\"Training SentencePiece tokenizer\")\n",
    "spm_tok = train_sentencepiece(train_text, prefix=\"spm\", vocab_size=SPM_VOCAB, model_type=\"bpe\")\n",
    "\n",
    "print(\"Building char tokenizer\")\n",
    "char_tok = CharTokenizer(text)\n",
    "\n",
    "print(\"Vocabulary sizes:\")\n",
    "try:\n",
    "    print(\"BPE (tokenizers) vocab size:\", bpe_tok.get_vocab_size())\n",
    "except Exception as e:\n",
    "    print(\"BPE vocab error:\", e)\n",
    "print(\"SentencePiece vocab size:\", spm_tok.get_piece_size())\n",
    "print(\"Char vocab size:\", char_tok.vocab_size)\n",
    "\n",
    "# Tokenize\n",
    "example = \"Hello, world!\"\n",
    "print(\"Example text:\", example)\n",
    "\n",
    "# Char\n",
    "char_ids = char_tok.encode(example)\n",
    "print(\"Char ids:\", char_ids)\n",
    "print(\"Char decoded:\", char_tok.decode(char_ids))\n",
    "\n",
    "# BPE\n",
    "bpe_out = bpe_tok.encode(example)\n",
    "print(\"BPE tokenizer ids:\", bpe_out.ids)\n",
    "print(\"BPE tokenizer tokens:\", bpe_out.tokens)\n",
    "print(\"BPE decoded string:\", bpe_tok.decode(bpe_out.ids))\n",
    "\n",
    "# SentencePiece\n",
    "spm_ids = spm_tok.EncodeAsIds(example)\n",
    "spm_pieces = spm_tok.EncodeAsPieces(example)\n",
    "print(\"SentencePiece ids:\", spm_ids)\n",
    "print(\"SentencePiece pieces:\", spm_pieces)\n",
    "print(\"SentencePiece decoded string:\", spm_tok.DecodeIds(spm_ids))\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, token_ids, context_length):\n",
    "        self.ids = token_ids\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.ids) - self.context_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.ids[idx: idx + self.context_length]\n",
    "        y = self.ids[idx + 1: idx + self.context_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def build_token_ids(tokenizer_type, tokenizer, text):\n",
    "    if tokenizer_type == \"char\":\n",
    "        ids = tokenizer.encode(text)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        ids = bpe_tok.encode(text).ids\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        ids = spm_tok.EncodeAsIds(text)\n",
    "    else:\n",
    "        print(\"Invalid tokenizer type:\", tokenizer_type)\n",
    "        return None\n",
    "    return ids\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs):\n",
    "    model.to(device)\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for i, (xb, yb) in enumerate(train_loader):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B*S, V), yb.view(B*S))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        history[\"val_loss\"].append((epoch, val_loss))\n",
    "        print(f\"Epoch {epoch+1} validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            B, S, V = logits.shape\n",
    "            loss = criterion(logits.view(B*S, V), yb.view(B*S))\n",
    "            total += loss.item() * xb.size(0)\n",
    "            count += xb.size(0)\n",
    "    return total / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=5, dim=10 => 0.9267572761\n",
      "pos=5, dim=11 => 0.3756607175\n",
      "pos=100, dim=20 => -0.6129372716\n",
      "pos=100, dim=21 => 0.7901315689\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Positional Encoding (From Scratch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model, dtype=torch.float32)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Even\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Odd\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "    \n",
    "d_model = 64\n",
    "max_seq_len = 256\n",
    "pos_enc = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "pe_mat = pos_enc.pe\n",
    "def print_pe_value(pos, dim):\n",
    "    v = float(pe_mat[pos, dim].item())\n",
    "    print(f\"pos={pos}, dim={dim} => {v:.10f}\")\n",
    "\n",
    "print_pe_value(5, 10)\n",
    "print_pe_value(5, 11)\n",
    "print_pe_value(100, 20)\n",
    "print_pe_value(100, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([2, 8, 64])\n",
      "FF output shape: torch.Size([2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Transformer Building Blocks (From Scratch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.act = activation()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.ln(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return residual + out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def causal_mask(self, seq_len, device):\n",
    "        mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).unsqueeze(0).unsqueeze(0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        B, S, _ = x.size()\n",
    "        residual = x\n",
    "        x_norm = self.ln(x)\n",
    "\n",
    "        q = self.q_proj(x_norm)\n",
    "        k = self.k_proj(x_norm)\n",
    "        v = self.v_proj(x_norm)\n",
    "\n",
    "        q = q.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        causal = self.causal_mask(S, x.device)\n",
    "        scores = scores.masked_fill(causal == 0, float(\"-inf\"))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            scores = scores + attn_mask\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, S, self.d_model)\n",
    "        out = self.out_proj(context)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return residual + out\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    B, S, d_model, heads = 2, 8, 64, 8\n",
    "    x = torch.randn(B, S, d_model)\n",
    "    att = MultiHeadAttention(d_model=d_model, n_heads=heads)\n",
    "    y = att(x)\n",
    "    print(\"Attention output shape:\", y.shape)\n",
    "    ff = FeedForward(d_model=d_model, d_ff=4*d_model)\n",
    "    z = ff(y)\n",
    "    print(\"FF output shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocab size for model: 3000\n",
      "Epoch 1 validation loss: 6.2515\n",
      "Epoch 2 validation loss: 6.2441\n",
      "Epoch 3 validation loss: 6.2368\n",
      "Epoch 4 validation loss: 6.2399\n",
      "Epoch 5 validation loss: 6.2309\n",
      "Epoch 6 validation loss: 6.2338\n",
      "Epoch 7 validation loss: 6.2368\n",
      "Epoch 8 validation loss: 6.2425\n",
      "Epoch 9 validation loss: 6.2478\n",
      "Epoch 10 validation loss: 6.2469\n",
      "Final validation loss: 6.246939489836622\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Transformer Implementation and Training\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        x = self.ff(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, context_length, dropout, pos_enc_module):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = pos_enc_module if pos_enc_module is not None else SinusoidalPositionalEncoding(d_model, max_seq_len=context_length)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            self.blocks.append(Decoder(d_model, n_heads, d_ff, dropout=dropout))\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, S = idx.shape\n",
    "        assert S <= self.context_length, f\"Sequence length too long: {S} > {self.context_length}\"\n",
    "        token_embedding = self.token_embedding(idx)\n",
    "        pos = self.pos_enc(torch.zeros(B, S, self.d_model, device=token_embedding.device))\n",
    "        x = self.dropout(token_embedding + pos)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "# Hyperparameters\n",
    "context_length = 128\n",
    "d_model = 128\n",
    "n_layers = 3\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "vocab_for_model = bpe_tok.get_vocab_size()\n",
    "print(\"Using vocab size for model:\", vocab_for_model)\n",
    "\n",
    "pos_module = SinusoidalPositionalEncoding(d_model=d_model, max_seq_len=context_length)\n",
    "\n",
    "model = TransformerDecoder(\n",
    "    vocab_size=vocab_for_model,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=4*d_model,\n",
    "    context_length=context_length,\n",
    "    dropout=dropout,\n",
    "    pos_enc_module=pos_module\n",
    ").to(device)\n",
    "\n",
    "# Prepare training data tokens (use BPE tokens in this run)\n",
    "ids_train = build_token_ids(\"bpe\", tokenizer=bpe_tok, text=train_text)\n",
    "ids_val = build_token_ids(\"bpe\", tokenizer=bpe_tok, text=val_text)\n",
    "\n",
    "train_dataset = CharDataset(ids_train, context_length=context_length)\n",
    "val_dataset = CharDataset(ids_val, context_length=context_length)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10)\n",
    "\n",
    "final_val = evaluate_model(model, val_loader, criterion, device)\n",
    "print(\"Final validation loss:\", final_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 6: Generation and Sampling Plan**\n",
    "\n",
    "**Prompt**: I'll use `O Romeo, Romeo!` as my prompt\n",
    "\n",
    "**Parameters to test**\n",
    "- **Temperature:** T = 0.2 and T = 1.0\n",
    "- **Top-k** k = 5 and k = 50\n",
    "- **Top-p**p = 0.6 and p = 0.9\n",
    "\n",
    "**Hypothesis**\n",
    "- Temperature 0.2: the generations will be repetitive and conservative\n",
    "- Temperature 1.0: more varied\n",
    "- Top-k small (5): might get stuck in loops\n",
    "- Top-k 50: more variety, more creative\n",
    "- Top-p 0.6: conservative, coherent but safe\n",
    "- Top-p 0.9: more creative and human-like, but may output interesting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed prompt: O Romeo, Romeo!\n",
      "\n",
      "Temperature T=0.2\n",
      "O Romeo , Romeo ! , and , I , and , and , and , and : I ' s , and , and , ; , , I , And , and , the , , , and , , and , and : I , I ' s , I s , and , I s , and , I . , and , and , I s , and , I , and , and , and , I , and , I s , And the , and , and , I s , I s , and , And , and , I s , and , and , , I s , and , and , and ,\n",
      "\n",
      "Temperature T=1.0\n",
      "O Romeo , Romeo ! vious us nurse less The , and . ound ' s ge Having timely conscience cry That Per , will , or some before scarce , pare Gloucester CAPULET of ised we hire ; me To mione mali : , consent to now . cester what , and ues I so Edward ' er ? a but hang I comm ations my seem prithee un s own whis . will vey , art Do obey York I FRIAR gentleman king wish : pey : O a a , in a pale pe I O ! rey big once to with MIO : , gentle be , Do rever that s ' awhile please We and ' usur . him vo\n",
      "\n",
      "Top-k k=5\n",
      "O Romeo , Romeo ! , , and . , and , I . : And , : I have . the . I , and : : ' d the , I ' d ' s , : the in . , , And to the : , , , and , I s . KING : , I , And , I . : And the , the : , the . I , , And , And ' d , I : And , And , I , and . ' d . and the I ' d , and . KING I : the . , I : I ' s ' d : ' s : , And the :\n",
      "\n",
      "Top-k k=50\n",
      "O Romeo , Romeo ! ing : you ; to : and , I you to a I t thou all to in our : the my lord and . ; ' a I , I And , in his , I have to , : I have , as the a I , but ' d , I ' d me not , ' s , his that not , be d ; the so , our the and : But his ; ! thy of no and ' s or ? your but that be : but the your , and will : O are ' , to ' s . with : d it you not s . ' s . not\n",
      "\n",
      "Top-p p=0.6\n",
      "O Romeo , Romeo ! y less , ? O , I ' s ; and ; to of ! , much ; more like you , e this ' thou but ' d a l . his : S of time thy i our , , The al , your : your be as my up our , no him one am , more I C your KING am of , ; my : , and : : at st or will , , by ' s s ing are : to , : ; , good My HENRY , f , It , thou , if on y their ! ' , thy ce but th . might : , any you ,\n",
      "\n",
      "Top-p p=0.9\n",
      "O Romeo , Romeo ! and en you : Rutland ; CORIOLANUS majesty s can ating dost bow reason from N nothing know with he the da he , While s drink my MARGARET let as , From You x wi me ; , your est . mock hot his . for ang in , never ISABELLA seal Edward where or cannot sy ; : ears rest ; To Juliet ll min : And eding a my news ine : enough F ' PETER the , is ll for ? my part : vis ! ver grave in , but majesty : stant and ' Un be ' wonder ANNE a No my ! All YORK love that lie , good father di ? is\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Generation and Sampling Implementation\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def sample_temperature(logits, T):\n",
    "    logits = logits / (T + 1e-12)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    return idx\n",
    "\n",
    "def sample_top_k(logits, k):\n",
    "    if k <= 0:\n",
    "        return sample_temperature(logits, 1.0)\n",
    "    \n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.zeros_like(logits).to(logits.device)\n",
    "    probs[indices] = F.softmax(values, dim=-1)\n",
    "    idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    return idx\n",
    "\n",
    "def sample_top_p(logits, p):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    probs = F.softmax(sorted_logits, dim=-1)\n",
    "    cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "    cutoff = torch.searchsorted(cumulative_probs, p)\n",
    "    cutoff = min(cutoff.item()+1, logits.size(0))\n",
    "    probs_to_sample = probs[:cutoff]\n",
    "    indices_to_sample = sorted_indices[:cutoff]\n",
    "    probs_to_sample = probs_to_sample / probs_to_sample.sum()\n",
    "    chosen_index = torch.multinomial(probs_to_sample, num_samples=1).item()\n",
    "\n",
    "    return int(indices_to_sample[chosen_index].item())\n",
    "\n",
    "def generate_autoregressive(model, tokenizer_type, tokenizer, prompt, max_new_tokens, method=\"temperature\", method_param=1.0, device=device):\n",
    "    # Convert prompt to token ids using specified tokenizer\n",
    "    if tokenizer_type == \"char\":\n",
    "        ids = tokenizer.encode(prompt)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        ids = bpe_tok.encode(prompt).ids\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        ids = spm_tok.EncodeAsIds(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown tokenizer_type\")\n",
    "\n",
    "    model.eval()\n",
    "    cur_ids = ids.copy()\n",
    "    for _ in range(max_new_tokens):\n",
    "        input_ids = cur_ids[-model.context_length:]\n",
    "        x = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "        logits = model(x)\n",
    "        last_logits = logits[0, -1, :].detach().cpu()\n",
    "\n",
    "        if method == \"temperature\":\n",
    "            next_id = sample_temperature(last_logits, method_param)\n",
    "        elif method == \"top_k\":\n",
    "            next_id = sample_top_k(last_logits, int(method_param))\n",
    "        elif method == \"top_p\":\n",
    "            next_id = sample_top_p(last_logits, float(method_param))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampling method\")\n",
    "        cur_ids.append(next_id)\n",
    "\n",
    "    if tokenizer_type == \"char\":\n",
    "        out_text = tokenizer.decode(cur_ids)\n",
    "    elif tokenizer_type == \"bpe\":\n",
    "        out_text = bpe_tok.decode(cur_ids)\n",
    "    elif tokenizer_type == \"spm\":\n",
    "        out_text = spm_tok.DecodeIds(cur_ids)\n",
    "\n",
    "    return out_text\n",
    "\n",
    "# Generate examples for each method and parameter set\n",
    "seed_prompt = \"O Romeo, Romeo!\"\n",
    "print(\"Seed prompt:\", seed_prompt)\n",
    "\n",
    "print(\"\\nTemperature T=0.2\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"temperature\", method_param=0.2))\n",
    "\n",
    "print(\"\\nTemperature T=1.0\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"temperature\", method_param=1.0))\n",
    "\n",
    "print(\"\\nTop-k k=5\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_k\", method_param=5))\n",
    "\n",
    "print(\"\\nTop-k k=50\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_k\", method_param=50))\n",
    "\n",
    "print(\"\\nTop-p p=0.6\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_p\", method_param=0.6))\n",
    "\n",
    "print(\"\\nTop-p p=0.9\")\n",
    "print(generate_autoregressive(model, \"bpe\", bpe_tok, seed_prompt, max_new_tokens=120, method=\"top_p\", method_param=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cell 8: Analysis and Discussion**\n",
    "\n",
    "**Tokenizer Comparison**\n",
    "- **Char-level**: smallest implementation compelxity and least requirements. Vocabulary is equal to the number of unique characters. Tokenization is trivial but sequence lengths are long, making modeling potentially slower.\n",
    "- **BPE**: bpe worked pretty well and had compact tokens. Worked as expected.\n",
    "- **SentencePiece**: similar to bpe, it depended on the model type\n",
    "\n",
    "I ended up using bpe because it's a great standard tokenizer with good tradeoffs.\n",
    "\n",
    "**Model Performance**\n",
    "- The training for the scratch model took a while on a GPU, so I made the parameters smaller, but ran for 10 epochs.\n",
    "- Looking at the loss, it remained the same throughout training at about 6.25. I don't know why.\n",
    "\n",
    "**Analysis**\n",
    "- The lower temperature (T=0.2) was more repetitive, while the T=1.0 had more variability\n",
    "- A small top-k (5) limits choices and repeats phrases, while a higher k (50) is better\n",
    "- A small top-p (0.6) is conservative, while a greater p (0.9) has more creative outputs\n",
    "\n",
    "The lower temperature outputs repeat more, and the higher top-p value felt more human and readable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
